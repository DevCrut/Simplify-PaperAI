version: "3.9"

services:
  qwen-coder-llm:
    image: vllm/vllm-openai:latest
    container_name: qwen-coder-llm
    restart: unless-stopped

    ports:
      - "8000:8000"

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

    # ðŸ‘‡ THIS is the important bit: CLI args for vLLM
    command:
      [
        "--model", "Qwen/Qwen2.5-Coder-3B-Instruct",
        "--dtype", "bfloat16",
        "--max-model-len", "16384",             # 8k context instead of huge default
        "--gpu-memory-utilization", "0.6",     # only use ~60% VRAM
        "--kv-cache-dtype", "fp8",             # optional: smaller KV cache
        "--max-num-seqs", "4",                # don't batch tons of requests
        "--host", "0.0.0.0",
        "--port", "8000"
      ]

    environment:
      APP_ENV: "prod"
      PAPER_NON_INTERACTIVE: "1"
      PAPER_FORCE_REBUILD: "1"
      PAPER_DATA_DIR: "/data"
      PAPER_LOCAL_REPO_DIR: "/data/creator-docs"
      VLLM_USAGE_SOURCE: "docker-compose"

    volumes:
      - ./hf_cache:/root/.cache/huggingface
