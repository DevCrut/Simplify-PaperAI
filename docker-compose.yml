version: "3.9"

services:
  qwen-coder-llm:
    image: vllm/vllm-openai:latest
    container_name: qwen-coder-llm
    restart: unless-stopped

    ports:
      - "8000:8000"   # exposes http://localhost:8000

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

    # For older Docker versions, you might need this instead of `deploy`:
    # runtime: nvidia
    # environment:
    #   NVIDIA_VISIBLE_DEVICES: all
    #   NVIDIA_DRIVER_CAPABILITIES: compute,utility

    environment:
      VLLM_MODEL: "Qwen/Qwen2.5-Coder-3B-Instruct"
      VLLM_HOST: "0.0.0.0"
      VLLM_PORT: "8000"
      VLLM_DTYPE: "float16"
      VLLM_MAX_MODEL_LEN: "16384"
      VLLM_GPU_MEMORY_UTILIZATION: "0.75"
      # VLLM_TRUST_REMOTE_CODE: "1"  # uncomment if Qwen complains

    volumes:
      - ./hf_cache:/root/.cache/huggingface
